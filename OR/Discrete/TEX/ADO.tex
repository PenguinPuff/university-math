\documentclass{article}	
% \usepackage[
%   height=2in,      % height of the text block
%   width=7in,       % width of the text block
%   top=100pt,        % distance of the text block from the top of the page
%   headheight=48pt, % height for the header block
%   headsep=30pt,    % distance from the header block to the text block
%                % ensure an integer number of lines
%            % show the values of the parameters in the log file
% ]{geometry}

\usepackage[margin=0.75in]{geometry}
\usepackage{fancyhdr}
\usepackage{duckuments}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage[OT1]{fontenc}
\usetikzlibrary{automata,positioning}
\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary{shapes.multipart}
\usepackage{hyperref}
\usepackage{amssymb}
\renewcommand{\familydefault}{\sfdefault}

\fancypagestyle{otherpages}{
    \fancyhf{}
    \fancyhead[LO]{Applied Discrete Optimization}
    \fancyhead[RO]{[WI000819]}
    \fancyfoot[RO]{Seite \thepage}
    \fancyfoot[LO]{Penguin}
}
\usepackage{amsmath}
\usepackage{lastpage} % To reference last page's number
\begin{document}
\begin{center}
    \begin{minipage}{0.15\textwidth}
        \includegraphics[width=\linewidth]{/home/penguin/Downloads/tum-logo/AGDM.png} 
    \end{minipage}
    \hfill
    \begin{minipage}{0.55\textwidth}
        \centering
        \textcolor[HTML]{165DB1}{\LARGE Applied Discrete Optimization} \\[1ex]
        \textcolor[HTML]{165DB1}{\LARGE Operations Research} \\[1ex]
    \end{minipage}
    \hfill
    \begin{minipage}{0.15\textwidth}
        \includegraphics[width=\linewidth]{/home/penguin/Downloads/tum-logo/tum-logo.png} 
    \end{minipage}
\end{center}
\noindent
\large 
\vspace{5mm}
\normalsize
\section*{Course Topics}
\begin{enumerate}
    \item Linear Programming (Simplex Method, Degeneracy, Duality Theory, Dual-Simplex Method)
    \item Column generation 
    \item Dantzig Wolfe decomposition
    \item Branch-and-Bound, Branch-and-Cut
    \item Bender's decomposition
    \item Matheuristics \& Metaheuristics
    \item Frank-Wolfe algorithm
    \item Parametric Flows
    \item Network Equilibria
    \item Lagrangean relaxation
    \item Reformulation linearization technique
    \item Combinatorial auctions
\end{enumerate}
\section*{Applications}
\begin{enumerate}
    \item Planning, routing, and scheduling in public transport
    \item Traffic paradoxes and route guidance
    \item Tendering transportation services in freight networks
    \item Resource-constrained project scheduling in manufacturing
    \item Virtual private network design in telecommunications
    \item Managing kidney exchange networks in health care
    \item Route guidance in road traffic networks
    \item Container transport with automated guided vehicles
    \item Compiler optimization
    \item Product portfolio management
\end{enumerate}
\newpage
\normalsize
\pagestyle{otherpages}
this section is an abstract from \textbf{Robert J. Vanderbei}'s \textbf{Linear Programming: Foundations and Extensions}
\section{Linear Programming}
\begin{equation}\label{eq:2.1}
    \begin{array}[]{rcl}
    \text{maximize} & 5x_{1}+4x_{2}+3x_{3} \\
    \text{subject to} & 2x_{1}+3x_{2}+\ x_{3}\leq\ 5 \\
    & 4x_{1}+\ x_{2}+2x_{3}\leq 11 \\
    & 3x_{1}+4x_{2}+2x_{3}\leq\ 8 \\
    & x_{1},\ x_{2},\ x_{3}\geq\ 0.
    \end{array}
    \end{equation}
    
    We start by adding so-called \emph{slack variables}. For each of the less-than inequalities in \eqref{eq:2.1} we introduce a new variable that represents the difference between the right-hand side and the left-hand side. For example, for the first inequality,
    
    \[ 2x_{1}+3x_{2}+x_{3}\leq 5, \]
    
    we introduce the slack variable \( w_{1} \) defined by
    
    \[ w_{1}=5-2x_{1}-3x_{2}-x_{3}. \]
    
    It is clear then that this definition of \( w_{1} \), together with the stipulation that \( w_{1} \) be nonnegative, is equivalent to the original constraint. We carry out this procedure for each of the less-than constraints to get an equivalent representation of the problem:
    
    \begin{equation}\label{eq:2.2}
    \begin{array}[]{rcl}
    \text{maximize} & \zeta= 5x_{1}+4x_{2}+3x_{3} \\
    \text{subject to} & w_{1}= 5-2x_{1}-3x_{2}-\ x_{3} \\
    & w_{2}=11-4x_{1}-\ x_{2}-2x_{3} \\
    & w_{3}= 8-3x_{1}-4x_{2}-2x_{3} \\
    & x_{1},\ x_{2},\ x_{3},\ w_{1},\ w_{2},\ w_{3}\geq\ 0.
    \end{array}
    \end{equation}
    
    Note that we have included a notation, \( \zeta \), for the value of the objective function, \( 5x_{1}+4x_{2}+3x_{3} \).
    
    The simplex method is an iterative process in which we start with a solution \( x_{1},x_{2},\ldots,w_{3} \) that satisfies the equations and nonnegativities in \eqref{eq:2.2} and then look for a new solution \( \bar{x}_{1},\bar{x}_{2},\ldots,\bar{w}_{3} \), which is better in the sense that it has a larger objective function value:
    
    \[ 5\bar{x}_{1}+4\bar{x}_{2}+3\bar{x}_{3}>5x_{1}+4x_{2}+3x_{3}. \]
    
    We continue this process until we arrive at a solution that can't be improved. This final solution is then an optimal solution.
    
    To start the iterative process, we need an initial feasible solution \( x_{1},x_{2},\ldots,w_{3} \). For our example, this is easy. We simply set all the original variables to zero and use the defining equations to determine the slack variables:
    
    \[ x_{1}=0,\quad x_{2}=0,\quad x_{3}=0,\quad w_{1}=5,\quad w_{2}=11,\quad w_{3}=8. \]
    
    The objective function value associated with this solution is \( \zeta=0 \).
    
    We now ask whether this solution can be improved. Since the coefficient of \( x_{1} \) is positive, if we increase the value of \( x_{1} \) from zero to some positive value, we will increase \( \zeta \). But as we change its value, the values of the slack variables will also change. We must make sure that we don't let any of them go negative. Since \( x_{2}=x_{3}=0 \), we see that \( w_{1}=5-2x_{1} \), and so keeping \( w_{1} \) nonnegative imposes the restriction that \( x_{1} \) must not exceed \( 5/2 \). Similarly, the nonnegativity of \( w_{2} \) imposes the bound that \( x_{1}\leq 11/4 \), and the nonnegativity of \( w_{3} \) introduces the bound that \( x_{1}\leq 8/3 \). Since all of these conditions must be met, we see that \( x_{1} \) cannot be made larger than the smallest of these bounds: \( x_{1}\leq 5/2 \). Our new, improved solution then is
    
    \[ x_{1}=\frac{5}{2},\quad x_{2}=0,\quad x_{3}=0,\quad w_{1}=0,\quad w_{2}=1,\quad w_{3}=\frac{1}{2}. \]
    
    This first step was straightforward. It is less obvious how to proceed. What made the first step easy was the fact that we had one group of variables that were initially zero and we had the rest explicitly expressed in terms of these. This property can be arranged even for our new solution. Indeed, we simply must rewrite the equations in \eqref{eq:2.2} in such a way that \( x_{1},w_{2},w_{3} \), and \( \zeta \) are expressed as functions of \( w_{1},x_{2} \), and \( x_{3} \). That is, the roles of \( x_{1} \) and \( w_{1} \) must be swapped. To this end, we use the equation for \( w_{1} \) in \eqref{eq:2.2} to solve for \( x_{1} \):
    
    \[ x_{1}=\frac{5}{2}-\frac{1}{2}w_{1}-\frac{3}{2}x_{2}-\frac{1}{2}x_{3}. \]
    
    The equations for \( w_{2},w_{3} \), and \( \zeta \) must also be doctored so that \( x_{1} \) does not appear on the right. The easiest way to accomplish this is to do so-called \emph{row operations} on the equations in \eqref{eq:2.2}. For example, if we take the equation for \( w_{2} \) and subtract two times the equation for \( w_{1} \) and then bring the \( w_{1} \) term to the right-hand side, we get
    
    \[ w_{2}=1+2w_{1}+5x_{2}. \]
    
    Performing analogous row operations for \( w_{3} \) and \( \zeta \), we can rewrite the equations in \eqref{eq:2.2} as
    
    \begin{equation}\label{eq:2.3}
    \begin{split}
    \zeta &= 12.5-2.5w_{1}-3.5x_{2}+0.5x_{3} \\
    x_{1} &= 2.5-0.5w_{1}-1.5x_{2}-0.5x_{3} \\
    w_{2} &= 1 + 2w_{1} + 5x_{2} \\
    w_{3} &= 0.5+1.5w_{1}+0.5x_{2}-0.5x_{3}.
    \end{split}
    \end{equation}
    
    Note that we can recover our current solution by setting the "independent" variables to zero and using the equations to read off the values for the "dependent" variables.
    
    Now we see that increasing \( w_{1} \) or \( x_{2} \) will bring about a \emph{decrease} in the objective function value, and so \( x_{3} \), being the only variable with a positive coefficient, is the only independent variable that we can increase to obtain a further increase in the objective function. Again, we need to determine how much this variable can be increased without violating the requirement that all the dependent variables remain nonnegative. This time we see that the equation for \( w_{2} \) is not affected by changes in \( x_{3} \), but the equations for \( x_{1} \) and \( w_{3} \) do impose bounds, namely \( x_{3}\leq 5 \) and \( x_{3}\leq 1 \), respectively. The latter is the tighter bound, and so the new solution is
    
    \[ x_{1}=2,\quad x_{2}=0,\quad x_{3}=1,\quad w_{1}=0,\quad w_{2}=1,\quad w_{3}=0. \]
    
    The corresponding objective function value is \( \zeta=13 \).
    
    Once again, we must determine whether it is possible to increase the objective function further and, if so, how. Therefore, we need to write our equations with \( \zeta,x_{1},w_{2} \), and \( x_{3} \) written as functions of \( w_{1},x_{2} \), and \( w_{3} \). Solving the last equation in \eqref{eq:2.3} for \( x_{3} \), we get
    
    \[ x_{3}=1+3w_{1}+x_{2}-2w_{3}. \]
    
    Also, performing the appropriate row operations, we can eliminate \( x_{3} \) from the other equations. The result of these operations is
    
    \begin{equation}\label{eq:2.4}
    \begin{split}
    \zeta &= 13 - w_{1} - 3x_{2} - w_{3} \\
    x_{1} &= 2 - 2w_{1} - 2x_{2} + w_{3} \\
    w_{2} &= 1 + 2w_{1} + 5x_{2} \\
    x_{3} &= 1 + 3w_{1} + x_{2} - 2w_{3}.
    \end{split}
    \end{equation}
    
    \subsection{The Simplex Method}
    
    Consider the general linear programming problem presented in standard form:
    
    \[
    \begin{array}[]{ll}
    \mbox{maximize} & \sum_{j=1}^{n}c_{j}x_{j} \\
    \mbox{subject to} & \sum_{j=1}^{n}a_{ij}x_{j}\leq b_{i} \\
    & x_{j}\geq 0
    \end{array}\quad\quad i=1,2,\ldots,m
    \]
    
    Our first task is to introduce slack variables and a name for the objective function value:
    
    \begin{equation}\label{eq:2.5}
    \begin{split}
    \zeta &= \sum_{j=1}^{n}c_{j}x_{j} \\
    w_{i} &= b_{i} - \sum_{j=1}^{n}a_{ij}x_{j} \qquad i=1,2,\ldots,m.
    \end{split}
    \end{equation}
    
    As we saw in our example, as the simplex method proceeds, the slack variables become intertwined with the original variables, and the whole collection is treated the same. Therefore, it is at times convenient to have a notation in which the slack variables are more or less indistinguishable from the original variables. So we simply add them to the end of the list of \( x \)-variables:
    
    \[ (x_{1},\ldots,x_{n},w_{1},\ldots,w_{m}) = (x_{1},\ldots,x_{n},x_{n+1},\ldots,x_{n+m}). \]
    
    That is, we let \( x_{n+i} = w_{i} \). With this notation, we can rewrite \eqref{eq:2.5} as
    
    \[
    \begin{split}
    \zeta &= \sum_{j=1}^{n}c_{j}x_{j} \\
    x_{n+i} &= b_{i} - \sum_{j=1}^{n}a_{ij}x_{j} \qquad i=1,2,\ldots,m.
    \end{split}
    \]
    
    This is the starting dictionary. As the simplex method progresses, it moves from one dictionary to another in its search for an optimal solution. Each dictionary has \( m \) basic variables and \( n \) nonbasic variables. Let \( \mathcal{B} \) denote the collection of indices from \( \{1,2,\ldots,n+m\} \) corresponding to the basic variables, and let \( \mathcal{N} \) denote the indices corresponding to the nonbasic variables. Initially, we have \( \mathcal{N} = \{1,2,\ldots,n\} \) and \( \mathcal{B} = \{n+1,n+2,\ldots,n+m\} \), but this of course changes after the first iteration. Down the road, the current dictionary will look like this:
    
    \begin{equation}\label{eq:2.6}
    \begin{split}
    \zeta &= \bar{\zeta} + \sum_{j \in \mathcal{N}} \bar{c}_{j}x_{j} \\
    x_{i} &= \bar{b}_{i} - \sum_{j \in \mathcal{N}} \bar{a}_{ij}x_{j} \qquad i \in \mathcal{B}.
    \end{split}
    \end{equation}
    
    Note that we have put bars over the coefficients to indicate that they change as the algorithm progresses.
    
    Within each iteration of the simplex method, exactly one variable goes from nonbasic to basic and exactly one variable goes from basic to nonbasic. We saw this process in our example, but let us now describe it in general.
    
    The variable that goes from nonbasic to basic is called the \emph{entering variable}. It is chosen with the aim of increasing \( \zeta \); that is, one whose coefficient is positive: \emph{pick \( k \) from \( \{j \in \mathcal{N} : \bar{c}_{j} > 0\} \)}. Note that if this set is empty, then the current solution is optimal. If the set consists of more than one element (as is normally the case), then we have a choice of which element to pick. There are several possible selection criteria, some of which will be discussed in the next chapter. For now, suffice it to say that we usually pick an index \( k \) having the largest coefficient (which again could leave us with a choice).
    
    The variable that goes from basic to nonbasic is called the \emph{leaving variable}. It is chosen to preserve nonnegativity of the current basic variables. Once we have decided that \( x_{k} \) will be the entering variable, its value will be increased from zero to a positive value. This increase will change the values of the basic variables:
    
    \[ x_{i} = \bar{b}_{i} - \bar{a}_{ik}x_{k}, \qquad i \in \mathcal{B}. \]
    
    We must ensure that each of these variables remains nonnegative. Hence, we require that
    
    \begin{equation}\label{eq:2.7}
    \bar{b}_{i} - \bar{a}_{ik}x_{k} \geq 0, \qquad i \in \mathcal{B}.
    \end{equation}
    
    Of these expressions, the only ones that can go negative as \( x_{k} \) increases are those for which \( \bar{a}_{ik} \) is positive; the rest remain fixed or increase. Hence, we can restrict our attention to those \( i \)'s for which \( \bar{a}_{ik} \) is positive. And for such an \( i \), the value of \( x_{k} \) at which the expression becomes zero is
    
    \[ x_{k} = \bar{b}_{i}/\bar{a}_{ik}. \]
    
    Since we don't want any of these to go negative, we must raise \( x_{k} \) only to the smallest of all of these values:
    
    \[ x_{k} = \min_{i \in \mathcal{B} : \bar{a}_{ik} > 0} \bar{b}_{i}/\bar{a}_{ik}. \]
    
    Therefore, with a certain amount of latitude remaining, the rule for selecting the leaving variable is \emph{pick \( l \) from \( \{i \in \mathcal{B} : \bar{a}_{ik} > 0 \text{ and } \bar{b}_{i}/\bar{a}_{ik} \text{ is minimal}\} \)}.
    
    The rule just given for selecting a leaving variable describes exactly the process by which we use the rule in practice. That is, we look only at those variables for which \( \bar{a}_{ik} \) is positive and among those we select one with the smallest value of the ratio \( \bar{b}_{i}/\bar{a}_{ik} \). There is, however, another, entirely equivalent, way to write this rule which we will often use. To derive this alternate expression we use the convention that \( 0/0 = 0 \) and rewrite inequalities \eqref{eq:2.7} as
    
    \[ \frac{1}{x_{k}} \geq \frac{\bar{a}_{ik}}{b_{i}}, \qquad i \in \mathcal{B} \]
    
    (we shall discuss shortly what happens when one of these ratios is an indeterminate form \( 0/0 \) as well as what it means if none of the ratios are positive). Since we wish to take the largest possible increase in \( x_{k} \), we see that
    
    \[ x_{k} = \left( \max_{i \in \mathcal{B}} \frac{\bar{a}_{ik}}{\bar{b}_{i}} \right)^{-1}. \]
    
    Hence, the rule for selecting the leaving variable is as follows: \emph{pick \( l \) from \( \{i \in \mathcal{B} : \bar{a}_{ik}/\bar{b}_{i} \text{ is maximal}\} \)}.
    
    The main difference between these two ways of writing the rule is that in one we minimize the ratio of \( \bar{a}_{ik} \) to \( \bar{b}_{i} \) whereas in the other we maximize the reciprocal ratio. Of course, in the minimize formulation one must take care about the sign of the \( \bar{a}_{ik} \)'s. In the remainder of this book we will encounter these types of ratios often. We will always write them in the maximize form since that is shorter to write, acknowledging full well the fact that it is often more convenient, in practice, to do it the other way.
    
    Once the leaving-basic and entering-nonbasic variables have been selected, the move from the current dictionary to the new dictionary involves appropriate row operations to achieve the interchange. This step from one dictionary to the next is called a \emph{pivot}.
    
    As mentioned above, there is often more than one choice for the entering and the leaving variables. Particular rules that make the choice unambiguous are called \emph{pivot rules}.
    
    \subsubsection{Initialization}
    
    In the previous section, we presented the simplex method. However, we only considered problems for which the right-hand sides were all nonnegative. This ensured that the initial dictionary was feasible. In this section, we shall discuss what one needs to do when this is not the case.
    
    Given a linear programming problem
    
    \[
    \begin{array}[]{ll}
    \text{maximize} & \sum_{j=1}^{n}c_{j}x_{j} \\
    \text{subject to} & \sum_{j=1}^{n}a_{ij}x_{j} \leq b_{i} \qquad i=1,2,\ldots,m \\
    & x_{j} \geq 0 \qquad j=1,2,\ldots,n,
    \end{array}
    \]
    
    the initial dictionary that we introduced in the preceding section was
    
    \[
    \begin{split}
    \zeta &= \sum_{j=1}^{n}c_{j}x_{j} \\
    w_{i} &= b_{i} - \sum_{j=1}^{n}a_{ij}x_{j} \qquad i=1,2,\ldots,m.
    \end{split}
    \]
    
    The solution associated with this dictionary is obtained by setting each \( x_{j} \) to zero and setting each \( w_{i} \) equal to the corresponding \( b_{i} \). This solution is feasible if and only if all the right-hand sides are nonnegative. But what if they are not? We handle this difficulty by introducing an \emph{auxiliary problem} for which
    
    \begin{enumerate}
    \item a feasible dictionary is easy to find and
    \item the optimal dictionary provides a feasible dictionary for the original problem.
    \end{enumerate}

    The auxiliary problem is

    \[
    \text{maximize } -x_0
    \]
    
    \[
    \text{subject to } \sum_{j=1}^{n} a_{ij}x_j - x_0 \leq b_i \quad i = 1, 2, \ldots, m
    \]
    
    \[
    x_j \geq 0 \quad j = 0, 1, \ldots, n.
    \]
    
    It is easy to give a feasible solution to this auxiliary problem. Indeed, we simply set \( x_j = 0 \), for \( j = 1, \ldots, n \), and then pick \( x_0 \) sufficiently large. It is also easy to see that the original problem has a feasible solution if and only if the auxiliary problem has a feasible solution with \( x_0 = 0 \). In other words, the original problem has a feasible solution if and only if the optimal solution to the auxiliary problem has objective value zero.
    
    Even though the auxiliary problem clearly has feasible solutions, we have not yet shown that it has an easily obtained feasible dictionary. It is best to illustrate how to obtain a feasible dictionary with an example:
    
    \[
    \text{maximize } -2x_1 - x_2
    \]
    
    \[
    \text{subject to } -x_1 + x_2 \leq -1
    \]
    
    \[
    -x_1 - 2x_2 \leq -2
    \]
    
    \[
    x_2 \leq 1
    \]
    
    \[
    x_1, \, x_2 \geq 0.
    \]
    
    The auxiliary problem is
    
    \[
    \text{maximize } -x_0
    \]
    
    \[
    \text{subject to } -x_1 + x_2 - x_0 \leq -1
    \]
    
    \[
    -x_1 - 2x_2 - x_0 \leq -2
    \]
    
    \[
    x_2 - x_0 \leq 1
    \]
    
    \[
    x_0, \, x_1, \, x_2 \geq 0.
    \]
    
    Next we introduce slack variables and write down an initial \emph{infeasible dictionary}:
    
    \[
    \begin{split}
    \xi &= -x_0 \\
    w_1 &= -1 + x_1 - x_2 + x_0 \\
    w_2 &= -2 + x_1 + 2x_2 + x_0 \\
    w_3 &= 1 - x_2 + x_0.
    \end{split}
    \]
    
    This dictionary is infeasible, but it is easy to convert it into a feasible dictionary. In fact, all we need to do is one pivot with variable \( x_{0} \) entering and the "most infeasible variable," \( w_{2} \), leaving the basis:
    
    \[
    \begin{split}
    \xi &= -2 + x_{1} + 2x_{2} - w_{2} \\
    w_1 &= 1 - 3x_{2} + w_{2} \\
    x_0 &= 2 - x_{1} - 2x_{2} + w_{2} \\
    w_3 &= 3 - x_{1} - 3x_{2} + w_{2}.
    \end{split}
    \]
    
    Note that we now have a feasible dictionary, so we can apply the simplex method as defined earlier in this chapter. For the first step, we pick \( x_{2} \) to enter and \( w_{1} \) to leave the basis:
    
    \[
    \begin{split}
    \xi &= -1.33 + x_{1} - 0.67w_{1} - 0.33w_{2} \\
    x_2 &= 0.33 - 0.33w_{1} + 0.33w_{2} \\
    x_0 &= 1.33 - x_{1} + 0.67w_{1} + 0.33w_{2} \\
    w_3 &= 2 - x_{1} + w_{1}.
    \end{split}
    \]
    
    Now, for the second step, we pick \( x_{1} \) to enter and \( x_{0} \) to leave the basis:
    
    \[
    \begin{split}
    \xi &= 0 - x_{0} \\
    x_2 &= 0.33 - 0.33w_{1} + 0.33w_{2} \\
    x_1 &= 1.33 - x_{0} + 0.67w_{1} + 0.33w_{2} \\
    w_3 &= 0.67 + x_{0} + 0.33w_{1} - 0.33w_{2}.
    \end{split}
    \]
    
    This dictionary is optimal for the auxiliary problem. We now drop \( x_{0} \) from the equations and reintroduce the original objective function:
    
    \[
    \zeta = -2x_{1} - x_{2} = -3 - w_{1} - w_{2}.
    \]
    
    Hence, the starting feasible dictionary for the original problem is
    
    \[
    \begin{split}
    \zeta &= -3 - w_{1} - w_{2} \\
    x_2 &= 0.33 - 0.33w_{1} + 0.33w_{2} \\
    x_1 &= 1.33 + 0.67w_{1} + 0.33w_{2} \\
    w_3 &= 0.67 + 0.33w_{1} - 0.33w_{2}.
    \end{split}
    \]
    
    As it turns out, this dictionary is optimal for the original problem (since the coefficients of all the variables in the equation for \( \zeta \) are negative), but we can't expect to be this lucky in general. All we normally can expect is that the dictionary so obtained will be feasible for the original problem, at which point we continue to apply the simplex method until an optimal solution is reached.
    
    The process of solving the auxiliary problem to find an initial feasible solution is often referred to as \emph{Phase I}, whereas the process of going from a feasible solution to an optimal solution is called \emph{Phase II}.
    
    \subsubsection{Unboundedness}
    
    In this section, we shall discuss how to detect when the objective function value is unbounded.
    
    Let us now take a closer look at the "leaving variable" computation: \emph{pick \( l \) from \( \{i \in \mathcal{B} : \bar{a}_{ik}/\bar{b}_{i} \) is maximal\}}. We avoided the issue before, but now we must face what to do if a denominator in one of these ratios vanishes. If the numerator is nonzero, then it is easy to see that the ratio should be interpreted as plus or minus infinity depending on the sign of the numerator. For the case of \( 0/0 \), the correct convention (as we'll see momentarily) is to take this as a zero.
    
    What if all of the ratios, \( \bar{a}_{ik}/\bar{b}_{i} \), are nonpositive? In that case, none of the basic variables will become zero as the entering variable increases. Hence, the entering variable can be increased indefinitely to produce an arbitrarily large objective value. In such situations, we say that the problem is \emph{unbounded}. For example, consider the following dictionary:
    
    \[
    \begin{split}
    \zeta &= 5 + x_{3} - x_{1} \\
    x_2 &= 5 + 2x_{3} - 3x_{1} \\
    x_4 &= 7 - 4x_{1} \\
    x_5 &= x_{1}.
    \end{split}
    \]
    
    The entering variable is \( x_{3} \) and the ratios are
    
    \[
    -2/5, \quad -0/7, \quad 0/0.
    \]
    
    Since none of these ratios is positive, the problem is unbounded.
    
    In the next chapter, we will investigate what happens when some of these ratios take the value \( +\infty \).
    
    \subsubsection{Geometry}
    
    When the number of variables in a linear programming problem is three or less, we can graph the set of feasible solutions together with the level sets of the objective function. From this picture, it is usually a trivial matter to write down the optimal solution. To illustrate, consider the following problem:
    
    \[
    \text{maximize } 3x_{1} + 2x_{2}
    \]
    
    \[
    \text{subject to } -x_{1} + 3x_{2} \leq 12
    \]
    
    \[
    x_{1} + x_{2} \leq 8
    \]
    
    \[
    2x_{1} - x_{2} \leq 10
    \]
    
    \[
    x_{1}, x_{2} \geq 0.
    \]
    
    Each constraint (including the nonnegativity constraints on the variables) is a half-plane. These half-planes can be determined by first graphing the equation one obtains by replacing the inequality with an equality and then asking whether or not some specific point that doesn't satisfy the equality (often \( (0,0) \) can be used) satisfies the inequality constraint. The set of feasible solutions is just the intersection of these half-planes. For the problem given above, this set is shown in Figure 2. Also shown are two level sets of the objective function. One of them indicates points at which the objective function value is \( 11 \). This level set passes through the middle of the set of feasible solutions. As the objective function value increases, the corresponding level set moves to the right. The level set corresponding to the case where the objective function equals \( 22 \) is the last level set that touches the set of feasible solutions.
\subsection{Duality Theory}
Associated with every linear program is another called its dual. The dual of this dual linear program is the original linear program (which is then referred to as the primal linear program). Hence, linear programs come in primal/dual pairs. It turns out that every feasible solution for one of these two linear programs gives a bound on the optimal objective function value for the other. These ideas are important and form a subject called duality theory, which is the topic we shall study in this chapter.

\subsubsection{Motivation--Finding Upper Bounds}

We begin with an example:

\[
\begin{array}{ll}
\text{maximize} & 4x_{1} + x_{2} + 3x_{3} \\
\text{subject to} & x_{1} + 4x_{2} \leq 1 \\
& 3x_{1} - x_{2} + x_{3} \leq 3 \\
& x_{1}, x_{2}, x_{3} \geq 0.
\end{array}
\]

Our first observation is that every feasible solution provides a lower bound on the optimal objective function value, $\zeta^{*}$. For example, the solution $(x_{1},x_{2},x_{3})=(1,0,0)$ tells us that $\zeta^{*} \geq 4$. Using the feasible solution $(x_{1},x_{2},x_{3})=(0,0,3)$, we see that $\zeta^{*} \geq 9$. But how good is this bound? Is it close to the optimal value? To answer, we need to give upper bounds, which we can find as follows. Let's multiply the first constraint by $2$ and add that to $3$ times the second constraint:

\[
\begin{array}{rcl}
2(x_{1} + 4x_{2}) & \leq & 2(1) \\
+ 3(3x_{1} - x_{2} + x_{3}) & \leq & 3(3) \\
\hline
11x_{1} + 5x_{2} + 3x_{3} & \leq & 11
\end{array}
\]

Now, since each variable is nonnegative, we can compare the sum against the objective function and notice that

\[
4x_{1} + x_{2} + 3x_{3} \leq 11x_{1} + 5x_{2} + 3x_{3} \leq 11.
\]

Hence, $\zeta^{*} \leq 11$. We have localized the search to somewhere between $9$ and $11$. These bounds leave a gap (within which the optimal solution lies), but they are better than nothing. Furthermore, they can be improved. To get a better upper bound, we again apply the same upper bounding technique, but we replace the specific numbers we used before with variables and then try to find the values of those variables that give us the best upper bound. So we start by multiplying the two constraints by nonnegative numbers, $y_{1}$ and $y_{2}$, respectively. The fact that these numbers are nonnegative implies that they preserve the direction of the inequalities. Hence,

\[
\begin{array}{rcl}
y_{1}(x_{1} + 4x_{2}) & \leq & y_{1} \\
+ y_{2}(3x_{1} - x_{2} + x_{3}) & \leq & 3y_{2} \\
\hline
(y_{1} + 3y_{2})x_{1} + (4y_{1} - y_{2})x_{2} + (y_{2})x_{3} & \leq & y_{1} + 3y_{2}
\end{array}
\]

If we stipulate that each of the coefficients of the $x_{i}$'s be at least as large as the corresponding coefficient in the objective function,

\[
\begin{array}{rcl}
y_{1} + 3y_{2} & \geq & 4 \\
4y_{1} - y_{2} & \geq & 1 \\
y_{2} & \geq & 3,
\end{array}
\]

then we can compare the objective function against this sum (and its bound):

\[
\begin{array}{rcl}
\zeta = 4x_{1} + x_{2} + 3x_{3} & \leq & (y_{1} + 3y_{2})x_{1} + (4y_{1} - y_{2})x_{2} + (y_{2})x_{3} \\
& \leq & y_{1} + 3y_{2}.
\end{array}
\]

We now have an upper bound, $y_{1} + 3y_{2}$, which we should minimize in our effort to obtain the best possible upper bound. Therefore, we are naturally led to the following optimization problem:

\[
\begin{array}{ll}
\text{minimize} & y_{1} + 3y_{2} \\
\text{subject to} & y_{1} + 3y_{2} \geq 4 \\
& 4y_{1} - y_{2} \geq 1 \\
& y_{2} \geq 3 \\
& y_{1}, y_{2} \geq 0.
\end{array}
\]

This problem is called the dual linear programming problem associated with the given linear programming problem. In the next section, we will define the dual linear programming problem in general.

\subsection{The Dual Problem}

Given a linear programming problem in standard form,

\begin{equation}\label{eq:5.1}
\begin{array}{ll}
\text{maximize} & \sum_{j=1}^{n}c_{j}x_{j} \\
\text{subject to} & \sum_{j=1}^{n}a_{ij}x_{j} \leq b_{i} \quad i=1,2,\ldots,m \\
& x_{j} \geq 0,
\end{array}
\end{equation}

the associated \emph{dual linear program} is given by

\[
\begin{array}{ll}
\text{minimize} & \sum_{i=1}^{m}b_{i}y_{i} \\
\text{subject to} & \sum_{i=1}^{m}y_{i}a_{ij} \geq c_{j} \quad j=1,2,\ldots,n \\
& y_{i} \geq 0.
\end{array}
\]

Since we started with \eqref{eq:5.1}, it is called the \emph{primal problem}. Our first order of business is to show that taking the dual of the dual returns us to the primal. To see this, we first must write the dual problem in standard form. That is, we must change the minimization into a maximization and we must change the first set of greater-than-or-equal-to constraints into less-than-or-equal-to. Of course, we must effect these changes without altering the problem. To change a minimization into a maximization, we note that to minimize something it is equivalent to maximize its negative and then negate the answer:

\[
\min \sum_{i=1}^{m}b_{i}y_{i} = -\max \left(-\sum_{i=1}^{m}b_{i}y_{i}\right).
\]

To change the direction of the inequalities, we simply multiply through by minus one. The resulting equivalent representation of the dual problem in standard form then is

\[
\begin{array}{ll}
-\text{maximize} & \sum_{i=1}^{m}(-b_{i})y_{i} \\
\text{subject to} & \sum_{i=1}^{m}(-a_{ij})y_{i} \leq (-c_{j}) \quad j=1,2,\ldots,n \\
& y_{i} \geq 0 \quad i=1,2,\ldots,m.
\end{array}
\]

Now we can take its dual:

\[
\begin{array}{ll}
-\text{minimize} & \sum_{j=1}^{n}(-c_{j})x_{j} \\
\text{subject to} & \sum_{j=1}^{n}(-a_{ij})x_{j} \geq (-b_{i}) \quad i=1,2,\ldots,m \\
& x_{j} \geq 0 \quad j=1,2,\ldots,n,
\end{array}
\]

which is clearly equivalent to the primal problem as formulated in \eqref{eq:5.1}.

\subsection{The Dual Simplex Method}

In this section, we study what happens if we apply the simplex method to the dual problem. As we saw in our discussion of the strong duality theorem, one can actually apply the simplex method to the dual problem without ever writing down the dual problem or its dictionaries. Instead, the so-called dual simplex method is seen simply as a new way of picking the entering and leaving variables in a sequence of primal dictionaries.

We begin with an example:

\[
\begin{array}{ll}
\text{maximize} & -x_{1} - x_{2} \\
\text{subject to} & -2x_{1} - x_{2} \leq 4 \\
& -2x_{1} + 4x_{2} \leq -8 \\
& -x_{1} + 3x_{2} \leq -7 \\
& x_{1}, x_{2} \geq 0.
\end{array}
\]

The dual of this problem is

\[
\begin{array}{ll}
\text{minimize} & 4y_{1} - 8y_{2} - 7y_{3} \\
\text{subject to} & -2y_{1} - 2y_{2} - y_{3} \geq -1 \\
& -y_{1} + 4y_{2} + 3y_{3} \geq -1 \\
& y_{1}, y_{2}, y_{3} \geq 0.
\end{array}
\]

Introducing variables $w_{i}$, $i=1,2,3$, for the primal slacks and $z_{j}$, $j=1,2$, for the dual slacks, we can write down the initial primal and dual dictionaries:

\[
\text{(P)} \quad 
\begin{array}{rcl}
\zeta &=& -x_{1} - x_{2} \\
w_{1} &=& 4 + 2x_{1} + x_{2} \\
w_{2} &=& -8 + 2x_{1} - 4x_{2} \\
w_{3} &=& -7 + x_{1} - 3x_{2}
\end{array}
\]

\[
\text{(D)} \quad 
\begin{array}{rcl}
-\xi &=& -4y_{1} + 8y_{2} + 7y_{3} \\
z_{1} &=& 1 - 2y_{1} - 2y_{2} - y_{3} \\
z_{2} &=& 1 - y_{1} + 4y_{2} + 3y_{3}.
\end{array}
\]

As before, we have recorded the negative of the dual objective function, since we prefer to maximize the objective function appearing in a dictionary. More importantly, note that the dual dictionary is feasible, whereas the primal one is not. This suggests that it would be sensible to apply the simplex method to the dual. Let us do so, but as we go we shall keep track of the analogous pivots applied to the primal dictionary. For example, the entering variable in the initial dual dictionary is $y_{2}$, and the leaving variable then is $z_{1}$. Since $w_{2}$ is complementary to $y_{2}$ and $x_{1}$ is complementary to $z_{1}$, we will use $w_{2}$ and $x_{1}$ as the entering/leaving variables in the primal dictionary. Of course, since $w_{2}$ is basic and $x_{1}$ is nonbasic, $w_{2}$ must be the leaving variable and $x_{1}$ the entering variable--i.e., the reverse of what we have for the complementary variables in the dual dictionary. The result of these pivots is

\[
\text{(P)} \quad 
\begin{array}{rcl}
\zeta &=& -4 - 0.5w_{2} - 3x_{2} \\
w_{1} &=& 12 + w_{2} + 5x_{2} \\
x_{1} &=& 4 + 0.5w_{2} + 2x_{2} \\
w_{3} &=& -3 + 0.5w_{2} - x_{2}
\end{array}
\]

\[
\text{(D)} \quad 
\begin{array}{rcl}
-\xi &=& 4 - 12y_{1} - 4z_{1} + 3y_{3} \\
y_{2} &=& 0.5 - y_{1} - 0.5z_{1} - 0.5y_{3} \\
z_{2} &=& 3 - 5y_{1} - 2z_{1} + y_{3}.
\end{array}
\]

Continuing to work on the dual, we now see that $y_{3}$ is the entering variable and $y_{2}$ leaves. Hence, for the primal we use $w_{3}$ and $w_{2}$ as the leaving and entering variable, respectively. After pivoting, we have

\[
\text{(P)} \quad 
\begin{array}{rcl}
\zeta &=& -7 - w_{3} - 4x_{2} \\
w_{1} &=& 18 + 2w_{3} + 7x_{2} \\
x_{1} &=& 7 + w_{3} + 3x_{2} \\
w_{2} &=& 6 + 2w_{3} + 2x_{2}
\end{array}
\]

\[
\text{(D)} \quad 
\begin{array}{rcl}
-\xi &=& 7 - 18y_{1} - 7z_{1} - 6y_{2} \\
y_{3} &=& 1 - 2y_{1} - z_{1} - 2y_{2} \\
z_{2} &=& 4 - 7y_{1} - 3z_{1} - 2y_{2}.
\end{array}
\]

Now we notice that both dictionaries are optimal.

Of course, in each of the above dictionaries, the table of numbers in each dual dictionary is the negative-transpose of the corresponding primal table. Therefore, we never need to write the dual dictionary; the dual simplex method can be entirely described in terms of the primal dictionaries. Indeed, first we note that the dictionary must be dual feasible. This means that all the coefficients of the nonbasic variables in the primal objective function must be nonpositive. Given this, we proceed as follows. First we select the leaving variable by picking that basic variable whose constant term in the dictionary is the most negative (if there are none, then the current dictionary is optimal). Then we pick the entering variable by scanning across this row of the dictionary and comparing ratios of the coefficients in this row to the corresponding coefficients in the objective row, looking for the largest negated ratio just as we did in the primal simplex method. Once the entering and leaving variable are identified, we pivot to the next dictionary and continue from there. The reader is encouraged to trace the pivots that we performed above to see how these rules guide us to the optimal solution.

the pivots in the above example, paying particular attention to how one determines the
entering and leaving variables by looking only at the primal dictionary.
\\[5mm]
\textbf{Problem 1}
The ``Stigler diet problem''\textsuperscript{1} is a classical optimisation problem and one of the first linear programming applications. Given a list of foods and nutrients, the goal is to find the least expensive combination of foods that satisfies the daily nutritional requirements of an average person. Although the original paper dates back to 1945, it still remains a relevant problem today, with applications ranging from designing food aid programs in low-income countries to modern meal planning apps.

For the purpose of the exercise, we will focus on a simplified version of the problem that only includes macronutrients -- carbohydrates, proteins and fats -- and some of the most commonly consumed foods and beverages in Germany (see Table 1). The minimum nutritional targets consist of approximately 320g of carbohydrates, 60g of protein and 60g of fat, based on recommendations for a 30-year-old, low-active man who weighs 75kg and is 1.80m tall\textsuperscript{2}.

\begin{table}[h]
\centering
\caption{Values per 100 grams.}
\begin{tabular}{lcccc}
\toprule
Food    & Carbohydrates & Proteins & Fats & Cost \\
\midrule
Beer    & 7g    & 2g    & 0g   & 0.20€ \\
Bread    & 60g    & 8g    & 1g   & 0.40€ \\
Butter    & 0g    & 0g    & 80g   & 1.50€ \\
Meat    & 0g    & 26g    & 15g   & 2.00€ \\
Potatoes & 40g    & 2g    & 0g   & 0.20€ \\
\bottomrule
\end{tabular}
\end{table}

Determine graphically the minimum cost and the corresponding quantities of foods for a diet exclusively based on the following pair of products:

\begin{enumerate}
    \item[(a)] Bread and beer,
    \item[(b)] Bread and butter,
    \item[(c)] Beer and potatoes,
    \item[(d)] Meat and potatoes.
\end{enumerate}
\textbf{Problem 2} 
Consider the following LP problem:

\[
\begin{aligned}
&\min \ 3x_1 + 20x_2 + 13x_3 + 3x_4 \\
&\text{s.t. } 3x_1 + x_2 \geq 1 \\
&\quad \ 2x_1 + 3x_2 + 3x_3 \geq 2 \\
&\quad \ 4x_2 + x_3 + x_4 \geq 1 \\
&\quad \ x_1 + 2x_2 + x_3 + x_4 \geq 1 \\
&\quad \ x_1, x_2, x_3, x_4 \geq 0.
\end{aligned}
\]

Is \( x^* = (1, 0, 0, 1)^T \) an optimal solution? Justify the answer by providing an optimality proof or showing why it is not optimal.
\newpage
\textbf{Problem 3}
Consider the following LP problem:

\[
\begin{aligned}
&\max \ x_1 + x_2 + x_3 \\
&\text{s.t. } 4x_1 + 2x_2 + x_3 \leq 2 \\
&\quad \ x_1 + 3x_2 + x_3 \leq 5 \\
&\quad \ x_1, x_2, x_3 \geq 0.
\end{aligned}
\]

Determine an optimal solution by using the simplex algorithm.

\end{document}
\end{document}

